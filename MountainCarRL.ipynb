{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "AI-HW5.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "ba9tDACYtLOm"
      },
      "source": [
        "# import requirements\n",
        "import gym\n",
        "import random\n",
        "import numpy as np\n",
        "from keras.models     import Sequential\n",
        "from keras.layers     import Dense\n",
        "from keras.optimizers import Adam\n",
        "import time"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7dmG5X-itQ2w"
      },
      "source": [
        "# set environment for MountainCar and define some parameters\n",
        "env = gym.make('MountainCar-v0')\n",
        "env.reset()\n",
        "limit_steps = 200\n",
        "score_requirement = -198\n",
        "num_games = 10000\n",
        "num_epochs = 10"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pZupLTuWth4l"
      },
      "source": [
        "# play game several times and generate training data\n",
        "def model_data_generation():\n",
        "    training_data = []\n",
        "    # scores better than \"score requirement\" will be saved in this array\n",
        "    best_scores = []\n",
        "    # play game \"num_games\" times to generate training data\n",
        "    for game in range(num_games):\n",
        "        # At start of game with no plays, score is 0\n",
        "        score = 0\n",
        "        # array for saving (state, action) where state is (position, velocity)\n",
        "        game_memory = []\n",
        "        # array for saving previous state \n",
        "        game_history = []\n",
        "        # this loop continues until 200 actions are taken or we win the game\n",
        "        for step_index in range(limit_steps):\n",
        "            # choose an action: left = 0, stay = 1, right = 2\n",
        "            action = random.randrange(0, 3)\n",
        "            # take a step for choosen action and save (current state, reward we get from that action, we achived the flag or not, extra information(!)) afterwards\n",
        "            state, reward, done, info = env.step(action)    # state: (position, velocity)\n",
        "            \n",
        "            # after second step is taken(game has a history), save that history and the action has been taken in our game memory\n",
        "            if len(game_history) > 0:\n",
        "                game_memory.append([game_history, action])\n",
        "            # set previous state with position we are and  velocity we have(for usage in next move)\n",
        "            game_history = state\n",
        "\n",
        "            # set a better reward for cases we are near the flag(x > -0.2)\n",
        "            if state[0] > -0.2:\n",
        "                reward = 1\n",
        "            # add the reward we got from that action to our game score\n",
        "            score += reward\n",
        "\n",
        "            # if we achived to the flag, we are done, end of game\n",
        "            if done:\n",
        "                break\n",
        "\n",
        "        # after playeing the game, save the score, if it is better than the score we want, and -> \n",
        "        if score >= score_requirement:\n",
        "            best_scores.append(score)\n",
        "            # -> add states we were and actions we have taken(in form of vector) to our traing data\n",
        "            for history, action in game_memory:\n",
        "                if action == 0:    # left\n",
        "                    output = [1, 0, 0]\n",
        "                elif action == 1:   # stay\n",
        "                    output = [0, 1, 0]\n",
        "                elif action == 2:   # right\n",
        "                    output = [0, 0, 1]\n",
        "                training_data.append([history, output])\n",
        "        # reset the environment for next game\n",
        "        env.reset()    \n",
        "    return training_data"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xpF5V0L7uWMo"
      },
      "source": [
        "# building a model and fitting training data to that\n",
        "def train_model(training_data):\n",
        "    # X: array of (position, velocity)\n",
        "    X = np.array([i[0] for i in training_data]).reshape(-1, len(training_data[0][0]))\n",
        "    # y: array of actions for related (position, velocity)\n",
        "    y = np.array([i[1] for i in training_data]).reshape(-1, len(training_data[0][1]))\n",
        "    # model building\n",
        "    model = Sequential()\n",
        "    model.add(Dense(128, input_dim=len(X[0]), activation='relu'))\n",
        "    model.add(Dense(52, activation='relu'))\n",
        "    model.add(Dense(len(y[0]), activation='linear'))\n",
        "    model.compile(loss='mse', optimizer=Adam())\n",
        "    # fit the model to our training data through \"num_epochs\" epochs\n",
        "    model.fit(X, y, epochs=num_epochs)\n",
        "    # model is ready to return\n",
        "    return model"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "53UBL7C5ufQY",
        "outputId": "6e4dab2a-a3ee-403b-a57c-12bcbc0b12f0"
      },
      "source": [
        "start_time = time.time()\n",
        "# prepare training data with playing game several times\n",
        "training_data = model_data_generation()\n",
        "# build a model and fit these training data to that\n",
        "trained_model = train_model(training_data)\n",
        "print(\"Training time:\", time.time() - start_time, \"s\")"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "374/374 [==============================] - 14s 2ms/step - loss: 0.2387\n",
            "Epoch 2/10\n",
            "374/374 [==============================] - 1s 2ms/step - loss: 0.2217\n",
            "Epoch 3/10\n",
            "374/374 [==============================] - 1s 2ms/step - loss: 0.2214\n",
            "Epoch 4/10\n",
            "374/374 [==============================] - 1s 2ms/step - loss: 0.2207\n",
            "Epoch 5/10\n",
            "374/374 [==============================] - 1s 2ms/step - loss: 0.2209\n",
            "Epoch 6/10\n",
            "374/374 [==============================] - 1s 2ms/step - loss: 0.2202\n",
            "Epoch 7/10\n",
            "374/374 [==============================] - 1s 2ms/step - loss: 0.2202\n",
            "Epoch 8/10\n",
            "374/374 [==============================] - 1s 2ms/step - loss: 0.2204\n",
            "Epoch 9/10\n",
            "374/374 [==============================] - 1s 2ms/step - loss: 0.2194\n",
            "Epoch 10/10\n",
            "374/374 [==============================] - 1s 2ms/step - loss: 0.2199\n",
            "Training time: 100.82101655006409 s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cC9pXQKr29Fx",
        "outputId": "52470d63-57bb-4245-8fd4-e9b0ed54de9b"
      },
      "source": [
        "start_time = time.time()\n",
        "actions = []\n",
        "score = 0\n",
        "history = []\n",
        "\n",
        "# this loop continues until 200 actions are taken or we win the game\n",
        "for step_index in range(limit_steps):\n",
        "    # without any history, a random action is taken\n",
        "    if len(history) == 0:\n",
        "        action = random.randrange(0, 3)\n",
        "    # with history, find the action maximises the game score\n",
        "    else:\n",
        "        action = np.argmax(trained_model.predict(history.reshape(-1, len(history)))[0])\n",
        "    \n",
        "    # add that action to the array of actions from the begining of the game\n",
        "    actions.append(action)\n",
        "    # take a step for that action and save (current state, reward we get from that action, we achived the flag or not, extra information(!)) afterwards\n",
        "    state, reward, done, info = env.step(action)\n",
        "    # save current (position, velocity) in our history\n",
        "    history = state\n",
        "    # add the reward we got from that action to our game score  \n",
        "    score += reward\n",
        "    \n",
        "    # if we achived to the flag, we are done, end of game\n",
        "    if done:\n",
        "        break\n",
        "\n",
        "\n",
        "print('Score:',score)\n",
        "print(\"Testing time:\", time.time() - start_time, \"s\")"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Score: -125.0\n",
            "Testing time: 4.5083723068237305 s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PrR9HFBsSz7d"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}